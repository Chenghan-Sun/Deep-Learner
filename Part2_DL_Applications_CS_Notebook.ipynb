{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Part2_DL_Applications_CS_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chenghan-Sun/Deep-Learner/blob/main/Part2_DL_Applications_CS_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBNfS7sm9LYp"
      },
      "source": [
        "# Deep Learning Problem Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnKFU3gm9LYp"
      },
      "source": [
        "## Part II Applied Problems -- 2.1 Text\n",
        "- References:\n",
        "  - https://www.tensorflow.org/tutorials/text/word_embeddings\n",
        "  - https://www.tensorflow.org/tutorials/text/word2vec#prepare_training_data_for_word2vec\n",
        "  - https://www.tensorflow.org/tutorials/text/text_classification_rnn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpdBUhFy9LYp",
        "outputId": "0dfe9944-78c8-456e-ecf6-15b1a587c79e"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os, sys\n",
        "import shutil\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# import warnings\n",
        "import tensorflow as tf\n",
        "print(f\"Version of tf = {tf.version.VERSION}\")\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Model, Sequential, layers\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "\n",
        "# test GPU setting\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(\"Available GPUs = \", len(physical_devices))\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version of tf = 2.3.0\n",
            "Available GPUs =  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8lYfPic-tcw"
      },
      "source": [
        "### 2.1.0 Text Dataset Loading "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSDLVP0--t9Q",
        "outputId": "0ad111f4-4603-458a-e25f-bd100e548ec0"
      },
      "source": [
        "# load the ‚ÄùLarge Movie Review Dataset\" from tf.Keras file utility\n",
        "\n",
        "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "\n",
        "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
        "                                    untar=True, cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
        "os.listdir(dataset_dir)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train', 'imdbEr.txt', 'test', 'imdb.vocab', 'README']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K_MX7IB-zME",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea012da4-944d-4a7e-fde8-075c1b41ecfd"
      },
      "source": [
        "# remove additional folders \n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "print(f\"Original training dir: {os.listdir(train_dir)}\")\n",
        "\n",
        "remove_dir = os.path.join(train_dir, 'unsup')\n",
        "shutil.rmtree(remove_dir)\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "print(f\"Cleaned training dir: {os.listdir(train_dir)}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original training dir: ['labeledBow.feat', 'urls_unsup.txt', 'unsupBow.feat', 'neg', 'urls_neg.txt', 'pos', 'urls_pos.txt', 'unsup']\n",
            "Cleaned training dir: ['labeledBow.feat', 'urls_unsup.txt', 'unsupBow.feat', 'neg', 'urls_neg.txt', 'pos', 'urls_pos.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmTmTifM_yc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c32570-77f4-44f5-ea17-e5d978375fe3"
      },
      "source": [
        "# create tf.data.Dataset using tf.keras.preprocessing.text_dataset_from_directory\n",
        "# Use the train directory to create both train and validation datasets with a split of 20% for validation.\n",
        "\n",
        "batch_size_train = 20000\n",
        "batch_size_val = 5000\n",
        "seed = 123\n",
        "\n",
        "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size_train, validation_split=0.2, \n",
        "    subset='training', seed=seed)\n",
        "\n",
        "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=batch_size_val, validation_split=0.2, \n",
        "    subset='validation', seed=seed)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Using 20000 files for training.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Using 5000 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQBbqcQrhAfx",
        "outputId": "76bba18e-759d-4607-e658-b3e5748f2fdd"
      },
      "source": [
        "# visualize a few texts from training dataset\n",
        "\n",
        "for text_batch, label_batch in train_ds.take(1):\n",
        "    for i in range(5):\n",
        "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 b\"Canadian director Vincenzo Natali took the art-house circuit by storm with the intriguing and astonishingly intelligent Cube, which is my personal favourite SF film of the 90s. It framed the basic conceit of a group of strangers trapped in a maze shaped like a giant cube, shot entirely on one set, and took this idea in fascinating directions. <br /><br />I've been eagerly awaiting Natali's follow-up, and although its taken five years for him to mount another project, I'm delighted to say it was worth the wait. Cypher is a fascinating exploration of one man's place in the world, and how through a completely logical chain of events, finds himself in a situation beyond his control.<br /><br />I don't want to reveal too much about the plot, because one of the joys of Cypher is the different avenues it takes us down. It is so refreshing in this day and age to see a SF film that has more than one idea in it's head. Cypher is such a film.<br /><br />Morgan Sullivan (Jeremy Northam), one of the blandest people to ever walk the planet, is hired by the company DigiCorp. They send him to different parts of America to record different seminars. To his bewilderment, they are unbelievably boring. Covering topics as mundane as shaving cream and cheese.<br /><br />While Morgan is waiting for one seminar, he runs into Rita Foster (an impeccably cast Lucy Liu), the definition of an ice maiden. She gives him the brush-off, but there is something to her he finds irresistible. That's not too surprising considering the dry marriage he is in. <br /><br />When Rita turns up at another one of Morgan's seminars, she tells him his life is not what it appears. And I'm not saying anything more about the plot. To do so would cheapen the impact the rest of the film has on us, as well as the tortuous path that's so much fun to follow.<br /><br />As with Cube, Natali shows quite a talent for encompassing seemingly ordinary people, taking them out of the familiar, and basically seeing what will happen when they're thrust into the unknown. And Cypher follows similar patterns. But it's not a carbon copy of Cube. It has it's own inspiration.<br /><br />Cypher is a film that has more in common with conspiracy thrillers and paranoia stories. One of the great things about Cypher is the way these themes creep into the story without your knowledge. When Morgan realises his false identity is a piece of a much larger puzzle, it's as much of a shock to us as it is to him.<br /><br />One thing that distinguishes Cypher from Cube is how much more polished it is. Where Cube was confined to a minimalist setting and a shoestring budget with a cast of unknowns, Cypher is also on a low budget, but Natali economises it as much as he can, allowing him to broaden the horizon, and launching Morgan on an amazing journey through the labyrinth of his own identity.<br /><br />Natali's direction is exceptional, with a deft hand on the reins. There are some amazing camera angles from above, such as the enormity of the DigiCorp building as a vast, robust office block in conjunction to the insignificant speck that is Morgan standing outside. All the colour appears to have been bled out of the picture, which compliments the tone of the film perfectly as a modern day film-noir.<br /><br />The acting is uniformly excellent throughout. Jeremy Northam is a sympathetic figure from his loveless marriage to questioning his own identity. His performance is excellent because it's so modulated. He literally seems to transform right before our very eyes. From a clinical, spineless wimp to a confident man who will do anything to preserve his new identity.<br /><br />David Hewlett puts in a welcome appearance who made such an impact in Cube. He resides in a secret silo that looks like it was borrowed from Men in Black. His scene is one of the best because it's an exercise in carefully calculated suspense and paranoia. He is a supposed expert in identifying double-agents, and it's a fantastic piece of writing, brilliantly acted by Hewlett. All he has to do is look at Morgan, and we're drawn into his complex mind game.<br /><br />But it's Lucy Liu who's the scene stealer here. Too often she is cast in films where her potential is not utilised to full effect. But in Cypher, she is finally given a character that fits her like a glove. Rita is an aloof, guarded femme fatale that Liu inhabits with relish. I perked up every time she appeared because she is always in control, and can reduce a room to silence by the power of her icy stare alone.<br /><br />Things come to a very gratifying end, that doesn't conclude on an ambiguous note the way Cube did. But Morgan deserves his happy ending. After he's been put through the ringer like this, I cheered for him in the final scene. It's a perfect final moment because it comes as a ray of sunshine after a gloomy 90 minutes.<br /><br />Cypher succeeds on all counts. Engaging, shocking, always entertaining, it's everything that Total Recall wanted to be but wasn't. And it comes as a refreshing antidote to the overwhelming and inexplicable Matrix.<br /><br />A fine follow-up from Natali. And now I'm a committed fan of the man. Superb stuff!\"\n",
            "1 b\"I gave this film 10 not because it is a superbly consistent movie, but for it's pure ability to evoke emotions in its audience. The story of one-woman's-struggle-against-all-odds is an old clich\\xc3\\xa9 by now, but very few films have carried it off with so much warmth and sincerity as The Color Purple.<br /><br />It also showed a different side to the African-American experience - showing that after slaves were granted freedom many fell into the ways of the hated 'white man' and were abusive of their own people. I find this an important point as it goes against the portray-white-on-black-violence-and-win-an-Oscar trend.<br /><br />Also the acting performances are superb - especially Oprah who I now have a new found respect for.<br /><br />Well worth watching - but keep some tissue handy.\"\n",
            "1 b'I admit to being somewhat jaded about the movie genre of a young child softening the heart of his/her reluctant guardian. I\\'ve seen enough of them \\xc2\\x97 Baby Boom, Kolya, About a Boy, Mostly Martha, and to some extent, Whale Rider \\xc2\\x97 to expect to be bored by the formula. What held my attention in The King of Masks was the grimness of the setting: small-town China in the 1930\\'s. Extreme poverty was the norm, and girl children were considered so worthless to poor parents that they killed them at birth or gave them to whomever would take them on the black market. When Wang discovers his purchased grandson, whom he\\'s nicknamed \"Doggie,\" is a granddaughter, he initially casts her out, even though she\\'s showed great promise as street-performing heir. Even after he reluctantly takes her back, he\\'s not too upset when she\\'s kidnapped. The film is gritty then, showing the lengths to which a young, street-smart girl had to go to survive in that society.<br /><br />The two lead performances are believable and beguiling in their societal context. In a Western society, one would expect at least a hint of resentment from Wang at not having achieved more material success. Wang so thoroughly accepts his station as a celebrated artist with low societal status, though, that I did, too. While Doggie exhibits a level of precociousness and cunning that would be suspect in a modern, suburban child, it\\'s completely believable in the context of a kid constantly in survival mode in a society that treats poor girls like garbage. And after learning that her previous seven owners have physically and mentally abused her, her fierce attachment to Wang makes perfect sense.<br /><br />The peek at small-town life in a foreign country, the naturalness of the two lead actors, the surprising plot twists, and of course, the heartwarming resolution all contribute to a very watchable film.'\n",
            "1 b\"For a long time, 'The Menagerie' was my favorite 'Star Trek' episode though in recent years it has been eclipsed by 'City on the Edge of Forever.' What I used to prefer about 'Menagerie' was that it's more hard-core Star Trek with this fascinating back-story to the then-current Trek storyline. I still think it's fairly ingenious the way Gene Roddenberry incorporated the original pilot into a two-part episode. Though the 'new' part of the story is largely an excuse for Kirk and a few others (and us) to watch the pilot, the idea of Spock being court-martialed is a clever one. You can poke holes in the plot if you want. For instance, given the Talosians' mind-control abilities and Captain Pike's condition, why is it even necessary to physically bring Pike back to their planet? And there are other confusing questions about Pike and Commodore Mendez... best to not think too hard about the details and just enjoy ST's only two-parter.\"\n",
            "0 b\"A truly frightening film. Feels as if it were made in the early '90s by a straight person who wanted to show that gays are good, normal, mainstream-aspiring people. Retrograde to the point of being offensive, LTR suggests that monogamy and marriage are the preferred path to salvation for sad, lonely, sex-crazed gays. Wow! Who knew? The supporting characters are caricatures of gay stereotypes (the effeminate buffoon, the bitter, lonely queen, the fag hag, etc.) and the main characters are milquetoast, middle-class, middlebrow clones, of little interest.<br /><br />As far as the romantic & ideological struggles of the main couple are concerned, there's not much to say: we've seen it all before, and done much better.\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmHz4hgRkRPD",
        "outputId": "75249f8f-3a7a-4b61-bf23-d4555758e7a9"
      },
      "source": [
        "type(train_ds)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.data.ops.dataset_ops.BatchDataset"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXrcmzxLGsyh",
        "outputId": "2be5a8fb-0f7c-4d8b-c0c7-19ad4bbf9642"
      },
      "source": [
        "for text_batch, label_batch in train_ds.take(2):\n",
        "    print(text_batch.shape)\n",
        "\n",
        "for text_batch, label_batch in val_ds.take(2):\n",
        "    print(text_batch.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000,)\n",
            "(5000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DzTKuWL9LYp"
      },
      "source": [
        "### 2.1.1 Visualization\n",
        "\n",
        "- Plot a sample of 1000 data points from the training set (500 positive and 500\n",
        "negative) using t-SNE, and color the points by sentiment label. Comment on any observed trends."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXZvBp3yhAiP"
      },
      "source": [
        "# convert the a batch of BatchDataset to iterable object\n",
        "\n",
        "sub_train_ds = train_ds.take(1)\n",
        "sub_train_ds_list = list(sub_train_ds.as_numpy_iterator())"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOEvLi20EnnU",
        "outputId": "5fa15a20-d538-4799-df67-88b3e9963be4"
      },
      "source": [
        "# two empty dicts with positive labels and negative labels respectively, \n",
        "# from training dataset\n",
        "pos_sample_train_dict = {}\n",
        "neg_sample_train_dict = {}\n",
        "\n",
        "for idx, (text, label) in enumerate(zip(sub_train_ds_list[0][0], sub_train_ds_list[0][1])):\n",
        "    if len(neg_sample_train_dict.keys()) < 500:\n",
        "        if label == 0:\n",
        "            neg_sample_train_dict[idx] = text\n",
        "\n",
        "    if len(pos_sample_train_dict.keys()) < 500:\n",
        "        if label == 1:\n",
        "            pos_sample_train_dict[idx] = text\n",
        "\n",
        "print(f\"Finish loading positive texts = {len(pos_sample_train_dict.keys())}, \\\n",
        "negative texts = {len(neg_sample_train_dict.keys())}.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish loading positive texts = 500, negative texts = 500.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "gEPOW7RboWJb",
        "outputId": "7ac5c374-77ce-4fe9-c5c2-1f68cb3dcbb3"
      },
      "source": [
        "# convert dict to df\n",
        "# concat the dfs, with label as feature\n",
        "# show the df of sample pos/neg texts data\n",
        "\n",
        "pos_sample_train_df = pd.DataFrame.from_dict(pos_sample_train_dict, orient='index')\n",
        "neg_sample_train_df = pd.DataFrame.from_dict(neg_sample_train_dict, orient='index')\n",
        "\n",
        "pos_sample_train_df.columns = ['text']\n",
        "neg_sample_train_df.columns = ['text']\n",
        "\n",
        "pos_sample_train_df['label'] = [1 for i in range(500)]\n",
        "neg_sample_train_df['label'] = [0 for i in range(500)]\n",
        "\n",
        "text_sample_train_df = pd.concat([pos_sample_train_df, neg_sample_train_df])\n",
        "\n",
        "# shuffle\n",
        "text_sample_train_df = text_sample_train_df.sample(frac=1) \n",
        "text_sample_train_df.head(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>951</th>\n",
              "      <td>b'There is a reason why this made for British ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>841</th>\n",
              "      <td>b\"my friend bought the movie for 5\\xc2\\x80 (it...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>b\"Tom and Butch Cat fight over the capture of ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>b'Slaughter High is about a boy named Marty. H...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>760</th>\n",
              "      <td>b'This movie shows how racist John Singleton i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  label\n",
              "951  b'There is a reason why this made for British ...      0\n",
              "841  b\"my friend bought the movie for 5\\xc2\\x80 (it...      0\n",
              "488  b\"Tom and Butch Cat fight over the capture of ...      1\n",
              "97   b'Slaughter High is about a boy named Marty. H...      1\n",
              "760  b'This movie shows how racist John Singleton i...      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtTP3BV3Enkk",
        "outputId": "b035a216-2db3-4bce-eeec-3a7671691fc6"
      },
      "source": [
        "# Use TF-IDF to obtain a numerical representation for the data\n",
        "# only consider the 5000 most frequently-occuring words in the dataset\n",
        "\n",
        "tfidf_config = TfidfVectorizer(min_df=0, max_df=1, use_idf=True, max_features=5000, ngram_range=(1,1))\n",
        "sample_feature_matrix = tfidf_config.fit_transform(text_sample_train_df['text'])\n",
        "print(f\"Training tf-idf feature matrix dimension = {sample_feature_matrix.shape}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training tf-idf feature matrix dimension = (1000, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8tS_W9mrvo8",
        "outputId": "2d785947-39ce-475e-f673-8fd09a159cc8"
      },
      "source": [
        "# Perform t-SNE to reduce the dimensionality down to 2 dimenions\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_fit = tsne.fit_transform(sample_feature_matrix)\n",
        "print(f\"Reduced feature dimension = {tsne_fit.shape}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reduced feature dimension = (1000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "GuT_NHvZ9LYp",
        "outputId": "64f60d1b-fe2c-4e01-bb1c-2bed5d1b0e62"
      },
      "source": [
        "# visualize the T-SNE plot \n",
        "\n",
        "tsne_df = pd.DataFrame()\n",
        "tsne_df['tsne-comonent-1'] = tsne_fit[:,0]\n",
        "tsne_df['tsne-comonent-2'] = tsne_fit[:,1]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(\n",
        "    x=\"tsne-comonent-1\", y=\"tsne-comonent-2\",\n",
        "    hue=text_sample_train_df['label'],\n",
        "    palette=sns.color_palette(\"hls\", 2),\n",
        "    data=tsne_df,\n",
        "    legend=\"full\",\n",
        "    alpha=0.3\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7fa2d52358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGhpOVWt6ElI"
      },
      "source": [
        "**Comment on any observed trends 2.1.1Ôºö**\n",
        "\n",
        "- From the above t-SNE analysis, we cannot really tell the cluster of sentiment labels by words in texts. The first two t-SNE components cannot reflect the True label of the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpLLklsb9LYp"
      },
      "source": [
        "### 2.1.2 Baseline models\n",
        "\n",
        "- Use TF-IDF to obtain a numerical representation for the data. Using\n",
        "10-fold cross-validation, report average validation accuracy and F1 scores for a Logistic Regression classifier\n",
        "using the TF-IDF vectors as input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUxRxu-69LYp",
        "outputId": "e25b0f0e-ff47-4590-e5d8-aded4fc6f315"
      },
      "source": [
        "# convert the full batches of BatchDataset of train and valid ds to iterable objects\n",
        "# perform the data conversion on train ds\n",
        "full_train_ds_list = list(train_ds.as_numpy_iterator())\n",
        "full_pos_train_ds_dict = {}\n",
        "full_neg_train_ds_dict = {}\n",
        "\n",
        "for idx, (text, label) in enumerate(zip(full_train_ds_list[0][0], full_train_ds_list[0][1])):\n",
        "    if label == 0:\n",
        "        full_neg_train_ds_dict[idx] = text\n",
        "\n",
        "    if label == 1:\n",
        "        full_pos_train_ds_dict[idx] = text\n",
        "\n",
        "print(f\"Finish loading positive texts for training dataset = {len(full_pos_train_ds_dict.keys())}, \\\n",
        "negative texts = {len(full_neg_train_ds_dict.keys())}.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish loading positive texts for training dataset = 9943, negative texts = 10057.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lKjIMDz9LYp",
        "outputId": "57a2a8a4-a955-4ee3-fb83-3e522d6d4c74"
      },
      "source": [
        "# perform the data conversion on valid ds\n",
        "\n",
        "full_valid_ds_list = list(val_ds.as_numpy_iterator())\n",
        "full_pos_valid_ds_dict = {}\n",
        "full_neg_valid_ds_dict = {}\n",
        "\n",
        "for idx, (text, label) in enumerate(zip(full_valid_ds_list[0][0], full_valid_ds_list[0][1])):\n",
        "    if label == 0:\n",
        "        full_neg_valid_ds_dict[idx] = text\n",
        "\n",
        "    if label == 1:\n",
        "        full_pos_valid_ds_dict[idx] = text\n",
        "\n",
        "print(f\"Finish loading positive for valid dataset texts = {len(full_pos_valid_ds_dict.keys())}, \\\n",
        "negative texts = {len(full_neg_valid_ds_dict.keys())}.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finish loading positive for valid dataset texts = 2557, negative texts = 2443.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuXSateJbmDy",
        "outputId": "ebec57d4-2e4b-4f0c-c25b-55db838caee2"
      },
      "source": [
        "# convert all full train/valid dicts to dfs\n",
        "# concat the dfs, with label as feature\n",
        "# show the dfs of sample pos/neg texts data\n",
        "num_train_ds = len(full_train_ds_list[0][1])  # 20000\n",
        "num_valid_ds = len(full_valid_ds_list[0][1])  # 5000\n",
        "\n",
        "# for train set\n",
        "full_pos_train_ds_df = pd.DataFrame.from_dict(full_pos_train_ds_dict, orient='index')\n",
        "full_neg_train_ds_df = pd.DataFrame.from_dict(full_neg_train_ds_dict, orient='index')\n",
        "# for valid set\n",
        "full_pos_valid_ds_df = pd.DataFrame.from_dict(full_pos_valid_ds_dict, orient='index')\n",
        "full_neg_valid_ds_df = pd.DataFrame.from_dict(full_neg_valid_ds_dict, orient='index')\n",
        "\n",
        "# edit column name \n",
        "full_pos_train_ds_df.columns = ['text']\n",
        "full_neg_train_ds_df.columns = ['text']\n",
        "full_pos_valid_ds_df.columns = ['text']\n",
        "full_neg_valid_ds_df.columns = ['text']\n",
        "\n",
        "# add label feature\n",
        "full_pos_train_ds_df['label'] = [1 for i in range(9943)]\n",
        "full_neg_train_ds_df['label'] = [0 for i in range(10057)]\n",
        "full_pos_valid_ds_df['label'] = [1 for i in range(2557)]\n",
        "full_neg_valid_ds_df['label'] = [0 for i in range(2443)]\n",
        "\n",
        "# concat pos/neg dfs\n",
        "full_train_df = pd.concat([full_pos_train_ds_df, full_neg_train_ds_df])\n",
        "full_valid_df = pd.concat([full_pos_valid_ds_df, full_neg_valid_ds_df])\n",
        "\n",
        "# shuffle train/valid dfs \n",
        "full_train_df = full_train_df.sample(frac=1) \n",
        "full_valid_df = full_valid_df.sample(frac=1)\n",
        "\n",
        "# show top 5 data\n",
        "print(full_train_df.head(5))\n",
        "print(full_valid_df.head(5))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                    text  label\n",
            "19140  b'In the questionable comedy vein of Mel Brook...      0\n",
            "973    b'I missed the first 10 or so minutes of the m...      0\n",
            "12982  b'Without \"mental anachronism\", this film whic...      1\n",
            "16224  b'Once in a while, you come upon a movie that ...      1\n",
            "19235  b'My favorite \"Imperialism\" movie and one of t...      1\n",
            "                                                   text  label\n",
            "615   b'This movie was on the Romance channel, and I...      0\n",
            "1465  b'Brilliant technology. But what good does it ...      0\n",
            "1994  b'\"Darkness\" was entertaining to some degree, ...      0\n",
            "1496  b'Despite its flaws, I enjoyed \"Cigarette Burn...      0\n",
            "2905  b'This movie is a lot of fun. The actors reall...      1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ9vE63E9LYp",
        "outputId": "f2f56059-641f-4bb4-8e80-3323cb3bc038"
      },
      "source": [
        "# Use TF-IDF to obtain a numerical representation for the data\n",
        "# only consider the 5000 most frequently-occuring words in the dataset\n",
        "# use the pre-configurated tf-dif setting in part 2.2.1\n",
        "\n",
        "full_train_feature_matrix = tfidf_config.fit_transform(full_train_df['text'])\n",
        "full_valid_feature_matrix = tfidf_config.fit_transform(full_valid_df['text'])\n",
        "print(f\"Train set tf-idf feature matrix dimension = {full_train_feature_matrix.shape}\")\n",
        "print(f\"Valid set tf-idf feature matrix dimension = {full_valid_feature_matrix.shape}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train set tf-idf feature matrix dimension = (20000, 5000)\n",
            "Valid set tf-idf feature matrix dimension = (5000, 5000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZU1M8tS2hzO",
        "outputId": "0bc625ce-0aa3-4fea-c5da-61c5cf3989d7"
      },
      "source": [
        "# train the baseline Logistic Regression classifier using the TF-IDF vectors as input\n",
        "\n",
        "# Config binary labelinzer\n",
        "lb = LabelBinarizer()\n",
        "\n",
        "# apply binarizer on train/valid set labels\n",
        "train_lb= lb.fit_transform(full_train_df['label']).flatten(order='C')\n",
        "valid_lb = lb.fit_transform(full_valid_df['label']).flatten(order='C')\n",
        "\n",
        "# config and fit baseline model LR with CV = 10\n",
        "baseline_LR = LogisticRegression()\n",
        "baseline_LR.fit(full_train_feature_matrix, train_lb)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdsUgsmt2h1_",
        "outputId": "688faeb9-39c2-4f7f-919e-252ebcd45cd7"
      },
      "source": [
        "# report average accuracy and F1 scores on validation set\n",
        "\n",
        "accuracy = cross_val_score(baseline_LR, full_valid_feature_matrix, valid_lb, cv=10)\n",
        "f1_score = cross_val_score(baseline_LR, full_valid_feature_matrix, valid_lb, cv=10, scoring='f1')\n",
        "print(f\"Average validation accuracy of the baseline Logit classifier = {np.mean(accuracy)}.\")\n",
        "print(f\"Average validation F1 score of the baseline Logit classifier = {np.mean(f1_score)}.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average validation accuracy of the baseline Logit classifier = 0.5114000000000001.\n",
            "Average validation F1 score of the baseline Logit classifier = 0.6767230806965906.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--0H05nebp1g"
      },
      "source": [
        "**Summary on 2.1.2**:\n",
        "\n",
        "\n",
        "For the baseline Logistic Regression classifier using the TF-IDF vectors as input: \n",
        "- Average validation accuracy = 0.5114\n",
        "- Average validation F1 scores = 0.6767"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmw2BP3d9LYp"
      },
      "source": [
        "### 2.1.3 Embeddings\n",
        "\n",
        "- Implement Word2Vec using Tensorflow, and train it on the text data to produce 32-dimensional word embeddings. \n",
        "- Plot the resulting word vectors using t-SNE. Plot the same 1000 observations as before (500 positive, 500 negative) using t-SNE, but represent each observation as the average of the embeddings for each word in the observation. \n",
        "- Perform the same cross-validation as above (Logistic Regression) and report performance (accuracy, F1), this time using the document-averaged embeddings as\n",
        "input. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLm-LpdRZ7v8"
      },
      "source": [
        "# add general configs\n",
        "\n",
        "SEED = 42 \n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j--lDPCKx0-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf51a96-1092-4608-e804-92c2842a33d8"
      },
      "source": [
        "# convert the train/valid datasets as text corpus\n",
        "# This part of generated .txt files were attached with this notebook\n",
        "\n",
        "train_cp_txt = open('train_corpus.txt', 'a')\n",
        "valid_cp_txt = open('valid_corpus.txt', 'a')\n",
        "\n",
        "for text_batch, label_batch in train_ds.take(1):\n",
        "    for i in tqdm.tqdm(range(20000)):\n",
        "        single_text = text_batch.numpy()[i]\n",
        "        print(single_text.decode(\"utf-8\"), '\\n', file=train_cp_txt)\n",
        "\n",
        "train_cp_txt.close()\n",
        "\n",
        "for text_batch, label_batch in val_ds.take(1):\n",
        "    for i in tqdm.tqdm(range(5000)):\n",
        "        single_text = text_batch.numpy()[i]\n",
        "        print(single_text.decode(\"utf-8\"), '\\n', file=valid_cp_txt)\n",
        "\n",
        "valid_cp_txt.close()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [08:31<00:00, 39.11it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:20<00:00, 239.16it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfvPbEw8x1Bw",
        "outputId": "1ca49de0-3afc-4906-c1b0-980b8ce03cbe"
      },
      "source": [
        "# show a few corpus examples\n",
        "# the pre-generated file was attached \n",
        "\n",
        "with open('/content/train_corpus.txt') as f: \n",
        "    lines = f.read().splitlines()\n",
        "for line in lines[:5]:\n",
        "    print(line)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I think part of the reason this movie was made...and is aimed at us gamers who actually play all the Nancy Drew PC games. There's been a lot of movies lately based on video games, and I think this in one of them.<br /><br />So this movie does not follow any book. But it does follow parts of the games. I buy and play every Nancy Drew games as soon as it comes out. And the games are from HerInteractive and are for \"girls who aren't afraid of a mouse!\" And some of these games actually won Parents' Choice Gold Awards. They are not only fun but you can actually learn a thing or two while playing.<br /><br />I took two of my step children with me to go see it and they loved it! The 10 yr. old had started playing her first Nancy Drew game a day before I took her to see the movie, and she was having so much fun playing the game I thought she would enjoy the movie as well. And I was right...she not only loved this movie but couldn't wait to get home to finish her first game and start another one.<br /><br />My other step daughter is only 7 and she also loved the movie but she is still a little to young too play the games yet, but she enjoys watching her sister play at times just to see what's going on.<br /><br />The games are based for children 10 yrs and older. All the games usually get pretty descent reviews and are classified as adventure games. For more information on the games just check out HerInterative Nancy Drew games. So personally I thought the movie was pretty good and I will buy it when it comes out on DVD. \n",
            "\n",
            "The fight scenes play like slow-motion Jackie Chan and the attempts at wit are pathetic (worst pun by far: \"Guess what? This time I heard you coming\"). The stars are a mismatched pair: Brandon Lee, despite the terrible lines he has to say, actually shows traces of charisma and screen charm - things that Dolph Lundgren is completely free of (at least in this movie). Note to the director: in the future, please stay away from any love scenes, especially when your main actress won't do any nudity and you have to rely extensively on a body double. (*1/2) \n",
            "\n",
            "This movie is definately one of my favourite movies in it's kind. The interaction between respectable and morally uncorruptable characters is an ode to chivalry and the honour code amongst thieves and policemen. It treats themes like duty, guilt, word, manipulation and trust like few films have done and, unfortunately, none that I can recall since the death of the 'policial' in the late seventies. The sequence is delicious, down to the essential, living nothing out and thus leading the spectator into a masterful plot right and wrong without accessory eye catching and spectacular scenes that are often needed in lesser specimens of the genre in order to keep the audience awake. No such scenes are present or needed. The argument is flowless and honest to the spectator, wich is an important asset in a genre in wich the the suspense is often achieved through the betrail of the audience. No, this is not miss Marble... A note of congratulations for the music is in order A film to watch and savour every minute, not just to see. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhSKJz5gTP8s"
      },
      "source": [
        "# Use the non empty lines to construct a tf.data.TextLineDataset object for next steps\n",
        "\n",
        "path_train_corpus = '/content/train_corpus.txt'\n",
        "path_valid_corpus = '/content/valid_corpus.txt'\n",
        "text_train_ds = tf.data.TextLineDataset(path_train_corpus).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
        "text_valid_ds = tf.data.TextLineDataset(path_train_corpus).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-b6SHqkTP6O"
      },
      "source": [
        "# Vectorize sentences from the corpus\n",
        "\n",
        "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
        "def custom_standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "    return tf.strings.regex_replace(stripped_html,\n",
        "                                    '[%s]' % re.escape(string.punctuation), '')\n",
        "    \n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 5000\n",
        "sequence_length = 250\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "\n",
        "# create vocabulary\n",
        "vectorize_layer.adapt(text_train_ds.batch(1024))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYMohLQqaFKl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8287254-1c0c-4210-8881-0c3516bba576"
      },
      "source": [
        "# Save the created vocabulary for reference\n",
        "\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', 'was', 'as', 'with', 'for', 'movie', 'but', 'film']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jsjEsvYTQAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2df6dc2-ed1d-4489-8dc0-e75928147bbf"
      },
      "source": [
        "# use the vectorize_layer to generate vectors for each element in the text train ds\n",
        "\n",
        "def vectorize_text(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return tf.squeeze(vectorize_layer(text))\n",
        "\n",
        "text_vector_ds = text_train_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(f\"The length of sequences = {len(sequences)}\")\n",
        "\n",
        "# see a few examples \n",
        "for seq in sequences[:1]:\n",
        "    print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The length of sequences = 20000\n",
            "[  10  102  166    5    2  274   11   17   13    1    7 3671   30  170\n",
            "    1   36  155  291   31    2 2634 2249    1 1480  215   74    4  168\n",
            "    5   91 4038  439   20  393 1480    3   10  102   11    8   28    5\n",
            "   93   37   11   17  120   21  811   98  284   18    9  120  811  510\n",
            "    5    2 1480   10  817    3  291  171 2634 2249 1480   14  505   14\n",
            "    9  253   44    3    2 1480   23   35    1    3   23   16  480   36\n",
            "  678 1592    5    4 3083    3   46    5  128 1480  155 1161  770 1079\n",
            " 1906 2091   34   23   21   60  242   18   22   68  155  799    4  149\n",
            "   41  105  134  380   10  549  105    5   56 1545  469   15   69    6\n",
            "  137   65    9    3   34  434    9    2  302    1  164   67  622  380\n",
            "   39   84 2634 2249  502    4  257  154   10  549   39    6   65    2\n",
            "   17    3   54   13  252   37   72  242  380    2  502   10  197   54\n",
            "   58  351    2   17   14   73    3   10   13    1   21   60  434   11\n",
            "   17   18  414  842    6   76  339    6 1343   39   84  502    3  363\n",
            "  151   28   56   80 1545  578    7   60 1159    3   54   79  434    2\n",
            "   17   18   54    7  126    4  110    6  179  100  291    2 1480  237\n",
            "   18   54 3662  145   39  784  291   30  210   40    6   65  706  165\n",
            "   20    2 1480   23  439   16  469  302    1    3  888   31] => ['i', 'think', 'part', 'of', 'the', 'reason', 'this', 'movie', 'was', '[UNK]', 'is', 'aimed', 'at', 'us', '[UNK]', 'who', 'actually', 'play', 'all', 'the', 'nancy', 'drew', '[UNK]', 'games', 'theres', 'been', 'a', 'lot', 'of', 'movies', 'lately', 'based', 'on', 'video', 'games', 'and', 'i', 'think', 'this', 'in', 'one', 'of', 'them', 'so', 'this', 'movie', 'does', 'not', 'follow', 'any', 'book', 'but', 'it', 'does', 'follow', 'parts', 'of', 'the', 'games', 'i', 'buy', 'and', 'play', 'every', 'nancy', 'drew', 'games', 'as', 'soon', 'as', 'it', 'comes', 'out', 'and', 'the', 'games', 'are', 'from', '[UNK]', 'and', 'are', 'for', 'girls', 'who', 'arent', 'afraid', 'of', 'a', 'mouse', 'and', 'some', 'of', 'these', 'games', 'actually', 'won', 'parents', 'choice', 'gold', 'awards', 'they', 'are', 'not', 'only', 'fun', 'but', 'you', 'can', 'actually', 'learn', 'a', 'thing', 'or', 'two', 'while', 'playing', 'i', 'took', 'two', 'of', 'my', 'step', 'children', 'with', 'me', 'to', 'go', 'see', 'it', 'and', 'they', 'loved', 'it', 'the', '10', '[UNK]', 'old', 'had', 'started', 'playing', 'her', 'first', 'nancy', 'drew', 'game', 'a', 'day', 'before', 'i', 'took', 'her', 'to', 'see', 'the', 'movie', 'and', 'she', 'was', 'having', 'so', 'much', 'fun', 'playing', 'the', 'game', 'i', 'thought', 'she', 'would', 'enjoy', 'the', 'movie', 'as', 'well', 'and', 'i', 'was', '[UNK]', 'not', 'only', 'loved', 'this', 'movie', 'but', 'couldnt', 'wait', 'to', 'get', 'home', 'to', 'finish', 'her', 'first', 'game', 'and', 'start', 'another', 'one', 'my', 'other', 'step', 'daughter', 'is', 'only', '7', 'and', 'she', 'also', 'loved', 'the', 'movie', 'but', 'she', 'is', 'still', 'a', 'little', 'to', 'young', 'too', 'play', 'the', 'games', 'yet', 'but', 'she', 'enjoys', 'watching', 'her', 'sister', 'play', 'at', 'times', 'just', 'to', 'see', 'whats', 'going', 'on', 'the', 'games', 'are', 'based', 'for', 'children', '10', '[UNK]', 'and', 'older', 'all']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_XyslD7TP-z"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "    # Elements of each training example are appended to these lists.\n",
        "    targets, contexts, labels = [], [], []\n",
        "\n",
        "    # Build the sampling table for vocab_size tokens.\n",
        "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "    # Iterate over all sequences (sentences) in dataset.\n",
        "    for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "        # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "            sequence, \n",
        "            vocabulary_size=vocab_size,\n",
        "            sampling_table=sampling_table,\n",
        "            window_size=window_size,\n",
        "            negative_samples=0)\n",
        "        \n",
        "        # Iterate over each positive skip-gram pair to produce training examples \n",
        "        # with positive context word and negative samples.\n",
        "        for target_word, context_word in positive_skip_grams:\n",
        "            context_class = tf.expand_dims(\n",
        "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "                true_classes=context_class,\n",
        "                num_true=1, \n",
        "                num_sampled=num_ns, \n",
        "                unique=True, \n",
        "                range_max=vocab_size, \n",
        "                seed=SEED, \n",
        "                name=\"negative_sampling\")\n",
        "\n",
        "            # Build context and label vectors (for one target word)\n",
        "            negative_sampling_candidates = tf.expand_dims(\n",
        "                negative_sampling_candidates, 1)\n",
        "\n",
        "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "            # Append each element from the training example to global lists.\n",
        "            targets.append(target_word)\n",
        "            contexts.append(context)\n",
        "            labels.append(label)\n",
        "\n",
        "    return targets, contexts, labels"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T88zBSoCkRM2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8c5d4d-7773-433a-9a60-87fcaa5f17d6"
      },
      "source": [
        "# Generate training examples from sequences\n",
        "\n",
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=2, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20000/20000 [12:16<00:00, 27.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1531171 1531171 1531171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gKRHgBq9LYp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1eb4ca9-abd6-42ea-f6cc-c4584fc02f9f"
      },
      "source": [
        "# Configure the dataset for performance\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "w2v_dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "w2v_dataset = w2v_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(w2v_dataset)\n",
        "\n",
        "w2v_dataset = w2v_dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(w2v_dataset)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n",
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1dOQfeB9LYp"
      },
      "source": [
        "# define the Word2Vec class\n",
        "\n",
        "class Word2Vec(Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_ns=4):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.target_embedding = Embedding(vocab_size, \n",
        "                                        embedding_dim,\n",
        "                                        input_length=1,\n",
        "                                        name=\"w2v_embedding\", )\n",
        "        self.context_embedding = Embedding(vocab_size, \n",
        "                                        embedding_dim, \n",
        "                                        input_length=num_ns+1)\n",
        "        self.dots = Dot(axes=(3,2))\n",
        "        self.flatten = Flatten()\n",
        "\n",
        "    def call(self, pair):\n",
        "        target, context = pair\n",
        "        we = self.target_embedding(target)\n",
        "        ce = self.context_embedding(context)\n",
        "        dots = self.dots([ce, we])\n",
        "        return self.flatten(dots)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXgbIPKkBWni",
        "outputId": "81c94eb7-8fc7-4a15-de51-cd2431a89a9a"
      },
      "source": [
        "# compile the word2vec model and training\n",
        "# generate weight matrices for each of the 5000 vocabularies\n",
        "# train on the text data to produce 32-dimensional word embeddings\n",
        "\n",
        "embedding_dim = 32\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# start training\n",
        "tensorboard_callback_w2v = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "word2vec.fit(w2v_dataset, epochs=10, callbacks=[tensorboard_callback_w2v])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "   1/1495 [..............................] - ETA: 0s - loss: 1.6091 - accuracy: 0.2383WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n",
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 0.0361s). Check your callbacks.\n",
            "1495/1495 [==============================] - 10s 7ms/step - loss: 1.4859 - accuracy: 0.3557\n",
            "Epoch 2/10\n",
            "1495/1495 [==============================] - 10s 7ms/step - loss: 1.3864 - accuracy: 0.4151\n",
            "Epoch 3/10\n",
            "1495/1495 [==============================] - 10s 7ms/step - loss: 1.3399 - accuracy: 0.4435\n",
            "Epoch 4/10\n",
            "1495/1495 [==============================] - 10s 7ms/step - loss: 1.3059 - accuracy: 0.4618\n",
            "Epoch 5/10\n",
            " 997/1495 [===================>..........] - ETA: 3s - loss: 1.2828 - accuracy: 0.4731"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-vkDxKrITuw"
      },
      "source": [
        "# get the weights of Embeddings\n",
        "\n",
        "# define a helper function for saving the trained weight matrix\n",
        "def gen_embeddings(sequences, weights, verbose=True):\n",
        "    \"\"\"implement average of the embeddings for each word in the observation\"\"\"\n",
        "    # initialize a container matrix for weights \n",
        "    shape_weights = weights.shape[1]\n",
        "    mat_embedding = np.zeros([len(sequences), shape_weights])\n",
        "    for seq in range(len(sequences)):\n",
        "        sequence = sequences[seq]\n",
        "        # average the 5000 most frequently-occuring words\n",
        "        weights_avg = np.mean(weights[sequence,:], axis=0)\n",
        "        mat_embedding[seq, :] = weights_avg\n",
        "    if verbose:\n",
        "        print(f\"The dimension of generated embeddings = {mat_embedding.shape}.\")\n",
        "    return mat_embedding\n",
        "\n",
        "# get trained weights from word2vec trained model and generate embeddings\n",
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "mat_embedding_train = gen_embeddings(sequences, weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk_SqTcNITxS"
      },
      "source": [
        "# in the later part, order to enhance the generalization of trained word2vec model,\n",
        "# use the trained vocab set to reconstruct the validation set\n",
        "\n",
        "# define a function to reconstruct validation sequence\n",
        "def reconstruct_validation_seq(val_text, vocab_train, length_seq):\n",
        "    \"\"\" use the trained vocab set from word2vec model to reconstruct \n",
        "    the validation set, return the tokenized encoded sentence from \n",
        "    validation set \n",
        "    \"\"\"\n",
        "    # tokenize the input text sentence\n",
        "    tokenizer = ToktokTokenizer()\n",
        "    tokens = tokenizer.tokenize(val_text)\n",
        "    single_val_sequence = []\n",
        "    for token in tokens:\n",
        "        index = np.where(np.array(vocab_train) == token)[0]\n",
        "        if len(index) != 0:\n",
        "            single_val_sequence.append(index[0])\n",
        "        # code 'UNK' denoted 1 after encoding \n",
        "        else:\n",
        "            single_val_sequence.append(1)\n",
        "    # length of sequence without encoded words \n",
        "    temp_length_seq = (length_seq - len(tokens))*[0]\n",
        "    # refill to reconstructed original sequence length\n",
        "    single_val_sequence = single_val_sequence + temp_length_seq\n",
        "\n",
        "    return single_val_sequence\n",
        "\n",
        "# provides the vocabulary to build a metadata file with one token per line\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "\n",
        "# perform  reconstruction\n",
        "val_sequences = []\n",
        "for text_batch, label_batch in val_ds.take(1):\n",
        "    for i in tqdm.tqdm(range(5000)):\n",
        "        single_text = text_batch.numpy()[i]\n",
        "        # decode the byte type \n",
        "        single_text = single_text.decode(\"utf-8\")\n",
        "        # call the function \n",
        "        single_val_sequence = reconstruct_validation_seq(\n",
        "            val_text=single_text,\n",
        "            vocab_train=vocab,\n",
        "            length_seq=sequence_length\n",
        "        )\n",
        "        val_sequences.append(single_val_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSfngDN4V5ot"
      },
      "source": [
        "# similar as above code cell for getting training embeddings\n",
        "# apply on validation set \n",
        "\n",
        "mat_embedding_valid = gen_embeddings(val_sequences, weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqoh4u30V60q"
      },
      "source": [
        "# Plot the resulting word vectors using t-SNE. \n",
        "# Perform t-SNE to reduce the dimensionality down to 2 dimenions\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_fit_w2v = tsne.fit_transform(mat_embedding_train[:1000])\n",
        "print(f\"Reduced word2vec model weights dimension = {tsne_fit_w2v.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H2JRQ2mV64-"
      },
      "source": [
        "# visualize the word2vec version of T-SNE plot \n",
        "\n",
        "tsne_df_w2v = pd.DataFrame()\n",
        "tsne_df_w2v['tsne-comonent-1'] = tsne_fit_w2v[:,0]\n",
        "tsne_df_w2v['tsne-comonent-2'] = tsne_fit_w2v[:,1]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(\n",
        "    x=\"tsne-comonent-1\", y=\"tsne-comonent-2\",\n",
        "    hue=text_sample_train_df['label'],\n",
        "    palette=sns.color_palette(\"hls\", 2),\n",
        "    data=tsne_df_w2v,\n",
        "    legend=\"full\",\n",
        "    alpha=0.3\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIvn4nKCBWuJ"
      },
      "source": [
        "# train the baseline Logistic Regression classifier using\n",
        "# the document-averaged embeddings as input\n",
        "\n",
        "# Config binary labelinzer\n",
        "lb_w2v = LabelBinarizer()\n",
        "\n",
        "# apply binarizer on train/valid set labels\n",
        "train_lb= lb_w2v.fit_transform(full_train_df['label']).flatten(order='C')\n",
        "valid_lb = lb_w2v.fit_transform(full_valid_df['label']).flatten(order='C')\n",
        "\n",
        "# config and fit baseline model LR with CV = 10\n",
        "baseline_LR_w2v = LogisticRegression()\n",
        "baseline_LR_w2v.fit(mat_embedding_train, train_lb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_tqGJg4BW3F"
      },
      "source": [
        "# report average accuracy and F1 scores on validation set\n",
        "\n",
        "accuracy_w2v = cross_val_score(baseline_LR_w2v, mat_embedding_valid, valid_lb, cv=10)\n",
        "f1_score_w2v = cross_val_score(baseline_LR_w2v, mat_embedding_valid, valid_lb, cv=10, scoring='f1')\n",
        "print(f\"Average validation accuracy of the Logit classifier with \\\n",
        "document-averaged embeddings = {np.mean(accuracy_w2v)}.\")\n",
        "print(f\"Average validation F1 score of the Logit classifier with \\\n",
        "document-averaged embeddings= {np.mean(f1_score_w2v)}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiWWciasZKir"
      },
      "source": [
        "**Summary on the Word2Vec Model 2.1.3**:\n",
        "\n",
        "1. The implementation of Word2Vec using Tensorflow was shown as all code blocks in this section 2.1.3, result in 32-dimensional word embeddings:\n",
        "    - The dimension of generated training embeddings = (20000, 32)\n",
        "    - The dimension of generated validation embeddings = (5000, 32).\n",
        "\n",
        "2. The plot of the resulting word vectors using t-SNE is shown as the figure above in this section, with dataset (500 positive, 500 negative), and represent each observation as the average of the embeddings for each word in the observation. \n",
        "\n",
        "2. For the Logistic Regression classifier with document-averaged embeddings: \n",
        "    - Average validation accuracy = 0.5114\n",
        "    - Average validation F1 scores = 0.6767"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT_x9S5V9LYp"
      },
      "source": [
        "### 2.1.4 RNN\n",
        "\n",
        "Truncate reviews to a maximum length of 250 words. Train a recurrent neural network on the training data with the following architecture:\n",
        "\n",
        "- LSTM layer, 100 units, tanh activation, sigmoid recurrent activation\n",
        "- Fully-connected layer, 1 unit, sigmoid activation\n",
        "\n",
        "Train with binary cross-entropy loss, using the Adam optimizer. Represent each token as the pretrained word embedding from the previous step. Use the last 10% of the training samples as validation. Do the following:\n",
        "\n",
        "- Print the number of trainable parameters in the model\n",
        "- Evaluate training and validation accuracy at the end of each epoch, and plot them as line plots on the same set of axes.\n",
        "- Evaluate accuracy on the test set.\n",
        "- Show an example from the test set for each class where the model misclassifies.\n",
        "- Comment on any other observations about the model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krkm7pxCZ7tr"
      },
      "source": [
        "# reload the train/valid datasets with new batch size = 64\n",
        "# shuffle the data for training and create batches of the (text, label) pairs\n",
        "BATCH_SIZE = 64\n",
        "seed = 123\n",
        "\n",
        "train_ds_rnn = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=BATCH_SIZE, validation_split=0.1, \n",
        "    subset='training', seed=seed)\n",
        "\n",
        "val_ds_rnn = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    'aclImdb/train', batch_size=BATCH_SIZE, validation_split=0.1, \n",
        "    subset='validation', seed=seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tu9M4Uc9LYp"
      },
      "source": [
        "# show a few examples\n",
        "\n",
        "for example, label in train_ds_rnn.take(1):\n",
        "    print('texts: ', example.numpy()[:3])\n",
        "    print('labels: ', label.numpy()[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imd2WhBH9LYp"
      },
      "source": [
        "# Configure the dataset for performance\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_ds_rnn = train_ds_rnn.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds_rnn = val_ds_rnn.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkqAr4-_ZROb"
      },
      "source": [
        "# Create the text encoder\n",
        "# Truncate reviews to a maximum length of 250 words\n",
        "# Vocabulary size and number of words in a sequence\n",
        "# VOCAB_SIZE = 5000\n",
        "# SEQUENCE_LENGTH = 250\n",
        "\n",
        "# # Create a custom standardization function to strip HTML break tags '<br />'.\n",
        "# def custom_standardization(input_data):\n",
        "#     lowercase = tf.strings.lower(input_data)\n",
        "#     stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
        "#     return tf.strings.regex_replace(stripped_html,\n",
        "#                                     '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# # Initialize a TextVectorization layer with the desired parameters to vectorize movie reviews\n",
        "# vectorize_layer = TextVectorization(\n",
        "#     standardize=custom_standardization,\n",
        "#     max_tokens=VOCAB_SIZE,\n",
        "#     output_mode = 'int',\n",
        "#     output_sequence_length=SEQUENCE_LENGTH)\n",
        "\n",
        "# # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "# text_ds = train_ds_rnn.map(lambda x, y: x)\n",
        "# vectorize_layer.adapt(text_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX0pjczejkqj"
      },
      "source": [
        "# show encoded text after vectorize_layer\n",
        "\n",
        "encoded_example = vectorize_layer(example)[:3].numpy()\n",
        "vocab = np.array(vectorize_layer.get_vocabulary())\n",
        "for n in range(3):\n",
        "    print(\"Original: \", example[n].numpy())\n",
        "    print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3D_d4PRjksy"
      },
      "source": [
        "# Create the RNN model\n",
        "\n",
        "rnn_model = tf.keras.Sequential([\n",
        "    vectorize_layer,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(vectorize_layer.get_vocabulary()),\n",
        "        output_dim=100),\n",
        "    # LSTM layer, 100 units, tanh activation, sigmoid recurrent activation\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(\n",
        "        100, activation='tanh', recurrent_activation='sigmoid')), \n",
        "    # Fully-connected layer, 1 unit, sigmoid activation\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXqoIgJVjkup"
      },
      "source": [
        "# Compile with binary cross-entropy loss, using the Adam optimizer\n",
        "\n",
        "rnn_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShsnmgcNZRVO"
      },
      "source": [
        "# Train the RNN model, save as history object\n",
        "\n",
        "history = rnn_model.fit(train_ds_rnn, epochs=30,\n",
        "                        validation_data=val_ds_rnn, \n",
        "                        validation_steps=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izohDHsnuaUR"
      },
      "source": [
        "# RNN model Summary \n",
        "# Print the number of trainable parameters in the model\n",
        "\n",
        "rnn_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skiq9mDctlqT"
      },
      "source": [
        "# plot training and validation accuracy at the end of each epoch\n",
        "\n",
        "# define a plot helper function\n",
        "def plot_graphs(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history['val_'+metric], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, 'val_'+metric])\n",
        "\n",
        "# plot the epoches accuracy \n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-TCOUd2tlsY"
      },
      "source": [
        "# Evaluate accuracy on the test set.\n",
        "\n",
        "test_loss, test_acc = rnn_model.evaluate(val_ds_rnn)\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqq_VbIIwlqo"
      },
      "source": [
        "# Show an example from the test set for each class where the model misclassifies\n",
        "\n",
        "# use trained RNN model to predict on one batch\n",
        "sample_val_ds_rnn = val_ds_rnn.take(1)\n",
        "sample_pred = rnn_model.predict(sample_val_ds_rnn)\n",
        "\n",
        "# record prediction results in a list\n",
        "pred_record = []\n",
        "for i in range(len(sample_pred)):\n",
        "    if sample_pred[i] < 0.5:\n",
        "        pred_record.append(0)\n",
        "    elif sample_pred[i] > 0.5:\n",
        "        pred_record.append(1)\n",
        "    else:\n",
        "        raise Exception(\"Neutral prediction happened!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPrfYe39wltA"
      },
      "source": [
        "# get the corresponding tests with True labels\n",
        "\n",
        "sample_valid_text_list = []\n",
        "valid_true_label_list = []\n",
        "for text, label in sample_val_ds_rnn:\n",
        "    sample_valid_text_list.append(text.numpy())\n",
        "    valid_true_label_list.append(label.numpy())\n",
        "\n",
        "# flatten the lists\n",
        "sample_valid_texts = np.array(sample_valid_text_list).flatten(order='C')\n",
        "valid_true_labels = np.array(valid_true_label_list).flatten(order='C')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD3QrDp_m_yI"
      },
      "source": [
        "# extract an example of False Positive text-label pair\n",
        "\n",
        "pos_misc_label_idx = np.where(valid_true_labels==1)[0][0]\n",
        "pos_misc_pred_label_idx = np.where(np.array(pred_record)==0)[0][0]\n",
        "\n",
        "pos_misc_text = sample_valid_texts[pos_misc_label_idx]\n",
        "pos_true_label = valid_true_labels[pos_misc_label_idx]\n",
        "pos_fake_label = pred_record[pos_misc_pred_label_idx]\n",
        "\n",
        "print(f\"The extracted False Negative text = {pos_misc_text}\")\n",
        "print(f\"True label of the text = {pos_true_label}\")\n",
        "print(f\"Pred label of the text = {pos_fake_label}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFiYTCkZnV3b"
      },
      "source": [
        "# extract an example of False Negative text-label pair\n",
        "\n",
        "neg_misc_label_idx = np.where(valid_true_labels==0)[0][0]\n",
        "neg_misc_pred_label_idx = np.where(np.array(pred_record)==1)[0][0]\n",
        "\n",
        "neg_misc_text = sample_valid_texts[neg_misc_label_idx]\n",
        "neg_true_label = valid_true_labels[neg_misc_label_idx]\n",
        "neg_fake_label = pred_record[neg_misc_pred_label_idx]\n",
        "\n",
        "print(f\"The extracted False Negative text = {neg_misc_text}\")\n",
        "print(f\"True label of the text = {neg_true_label}\")\n",
        "print(f\"Pred label of the text = {neg_fake_label}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ4mbLP9wmbD"
      },
      "source": [
        "**Answers of questions for RNN training 2.1.4**:\n",
        "\n",
        "1. Number of trainable parameters in the RNN baseline model = 661,001, as shown in model summary above.\n",
        "\n",
        "2. Plot of training and validation accuracy at the end of each epoch was shown as above code block.\n",
        "\n",
        "3. Accuracy on the test set = 0.9175, as shown above.\n",
        "\n",
        "4. Example from the test set for each class (positive/negative) where the model misclassifies was shown as upper two code blocks above.\n",
        "\n",
        "5. Comment on any other observations about the model performance:\n",
        "    - Above was the RNN baseline model which only using one LSTM layer. If stacking two or more LSTM layers, I would expect the model to achieve higher accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJgHyCEHfCai"
      },
      "source": [
        "- - - - - -  - - - - - - --  -- - - - -- - - -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yazpdz5Ye_mb"
      },
      "source": [
        "## Part II Applied Problems -- 2.2 Image\n",
        "\n",
        "- References:\n",
        "  - https://blog.keras.io/building-autoencoders-in-keras.html\n",
        "  - https://blog.tensorflow.org/2018/04/fashion-mnist-with-tfkeras.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ6QH_QfgB1G"
      },
      "source": [
        "### 2.2.0 Loading Fashion MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_Cza-26w5-q"
      },
      "source": [
        "# load Fashion MNIST dataset with training set and test set\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "print(\"x_train data shape:\", x_train.shape)\n",
        "print(\"y_train label shape:\", y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yupwmrwZgPYn"
      },
      "source": [
        "This is a dataset of 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gV-eTOfCfAAI"
      },
      "source": [
        "# visualize the first 25 images from the training set\n",
        "plt.rcParams['image.cmap'] = 'Greys'\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[y_train[i]])\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLmoGsEcfSnc"
      },
      "source": [
        "# pre-processing\n",
        "# Normalize the data \n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "print(f\"Original x training data size = {x_train.shape}.\")\n",
        "print(f\"Original x test data size = {x_test.shape}.\")\n",
        "\n",
        "# reshape with suitable input format\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "print(f\"Normalized and reshaped x training data size = {x_train.shape}.\")\n",
        "print(f\"Normalized and reshaped x test data size = {x_test.shape}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fi-6naKg0nj"
      },
      "source": [
        "### 2.2.1 Autoencoder\n",
        "\n",
        "- Train a fully-connected autoencoder with ReLU activation and layer sizes (784, 300, 100,3 00, 784) for 10 epochs. \n",
        "- Use mean squared error (MSE) loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NjXNmisfSst"
      },
      "source": [
        "# Build of a fully-connected autoencoder model\n",
        "\n",
        "def autoencoder_model(input_shape):\n",
        "    \"\"\" Build of a fully-connected autoencoder with ReLU activation \n",
        "    and layer sizes (784, 300, 100, 300, 784); Use mean squared error (MSE) loss. \n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=input_shape))\n",
        "    \n",
        "    # set encoders\n",
        "    model.add(layers.Dense(300, activation='relu'))\n",
        "    encoded = layers.Dense(100, activation='relu')\n",
        "    model.add(encoded)\n",
        "    \n",
        "    # set decoders\n",
        "    model.add(layers.Dense(300, activation='relu'))\n",
        "    decoded = layers.Dense(784, activation='sigmoid')\n",
        "    model.add(decoded)\n",
        "    \n",
        "    # compile the model\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss='mean_squared_error', \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WftaztDhfSyl"
      },
      "source": [
        "# model info\n",
        "autoencoder = autoencoder_model(input_shape=(784,))\n",
        "autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kJfASUafS4L"
      },
      "source": [
        "# train the auto encoder with 10 epochs without labels\n",
        "\n",
        "autoencoder.fit(x_train, \n",
        "                x_train,\n",
        "                batch_size=256,\n",
        "                epochs=10,\n",
        "                shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmv1qF_zfS9i"
      },
      "source": [
        "# Make predictions\n",
        "\n",
        "decoded_mnist = autoencoder.predict(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wkWs60TfTDv"
      },
      "source": [
        "# visualize the reconstructed inputs and the decoded images representations\n",
        "plt.figure(figsize=(16, 4))\n",
        "\n",
        "for i_class in range(len(class_names)):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, len(class_names), i_class + 1)\n",
        "    i = np.argwhere(y_train == i_class)[1, 0]\n",
        "    plt.imshow(x_train[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, len(class_names), i_class + 1 + len(class_names))\n",
        "    plt.imshow(decoded_mnist[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    plt.xlabel(class_names[i_class])\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfISmlncfTCW"
      },
      "source": [
        "# training the encoder with half of the autoencoder architecture\n",
        "encoder = keras.Sequential()\n",
        "encoder.add(layers.InputLayer(input_shape=(784,)))\n",
        "encoder.add(layers.Dense(300, activation='relu'))\n",
        "encoded = layers.Dense(100, activation='relu')\n",
        "encoder.add(encoded)\n",
        "\n",
        "encoded_mnist = encoder.predict(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ageZpV-fTAo"
      },
      "source": [
        "# plot the encodings for ten examples of input, each class as 10x10 heatmaps\n",
        "\n",
        "plt.figure(figsize=(16, 4))\n",
        "\n",
        "for i_class in range(len(class_names)):\n",
        "    i = np.argwhere(y_train == i_class)[1, 0]\n",
        "    ax = plt.subplot(1, len(class_names), i_class + 1)\n",
        "    plt.imshow(encoded_mnist[i].reshape((10, 10)))\n",
        "    plt.gray()\n",
        "    plt.xlabel(class_names[i_class])\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S344v9oRh4Wm"
      },
      "source": [
        "**Summary 2.2.1**: \n",
        "- From the reconstructed heatmaps for each class of Fashion MNIST dataset, We are losing quite a bit of detail with this basic encoder approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAh7qgwWiSYw"
      },
      "source": [
        "### 2.2.2 Dropout\n",
        "\n",
        "- Retrain the autoencoder with dropout applied to incoming weights to all layers except the input and output layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3G4tV_OfS8M"
      },
      "source": [
        "# define the autoencoder with dropout layers model\n",
        "\n",
        "def autoencoder_dropout_model(input_shape):\n",
        "    \"\"\" Build of a fully-connected autoencoder with ReLU activation \n",
        "    and layer sizes (784, 300, 100, 300, 784); Use mean squared error (MSE) \n",
        "    loss, with dropout applied to incoming weights to all layers \n",
        "    except the input and output layers\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.InputLayer(input_shape=input_shape))\n",
        "    \n",
        "    # set encoders with dropout\n",
        "    model.add(layers.Dense(300, activation='relu'))\n",
        "    model.add(layers.Dropout(0.25))  # first dropout layer\n",
        "    encoded = layers.Dense(100, activation='relu')\n",
        "    model.add(encoded)\n",
        "    model.add(layers.Dropout(0.25))  # second dropout layer\n",
        "    \n",
        "    # set decoders\n",
        "    model.add(layers.Dense(300, activation='relu'))\n",
        "    model.add(layers.Dropout(0.50))  # third dropout layer\n",
        "    decoded = layers.Dense(784, activation='sigmoid')\n",
        "    model.add(decoded)\n",
        "    \n",
        "    # compile the model\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss='mean_squared_error', \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XU09CPafS2n"
      },
      "source": [
        "# model info\n",
        "\n",
        "autoencoder_dropout = autoencoder_dropout_model(input_shape=(784,))\n",
        "autoencoder_dropout.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wzPVXQtfS1Z"
      },
      "source": [
        "# train the with-dropout auto encoder with 10 epochs without labels\n",
        "\n",
        "autoencoder_dropout.fit(x_train, \n",
        "                x_train,\n",
        "                batch_size=256,\n",
        "                epochs=10,\n",
        "                shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bh-GwqS0fSxT"
      },
      "source": [
        "# Make predictions\n",
        "\n",
        "decoded_mnist = autoencoder_dropout.predict(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9o-taXRfSvL"
      },
      "source": [
        "# visualize the reconstructed inputs and the decoded images representations\n",
        "plt.figure(figsize=(16, 4))\n",
        "\n",
        "for i_class in range(len(class_names)):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, len(class_names), i_class + 1)\n",
        "    i = np.argwhere(y_train == i_class)[1, 0]\n",
        "    plt.imshow(x_train[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, len(class_names), i_class + 1 + len(class_names))\n",
        "    plt.imshow(decoded_mnist[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    plt.xlabel(class_names[i_class])\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns-QzH3SijFR"
      },
      "source": [
        "# training the encoder with half of the autoencoder architecture\n",
        "encoder = keras.Sequential()\n",
        "encoder.add(layers.InputLayer(input_shape=(784,)))\n",
        "encoder.add(layers.Dense(300, activation='relu'))\n",
        "encoder.add(layers.Dropout(0.25))\n",
        "encoded = layers.Dense(100, activation='relu')\n",
        "encoder.add(encoded)\n",
        "\n",
        "encoded_mnist = encoder.predict(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUrFby8zijNd"
      },
      "source": [
        "# plot the encodings for ten examples of input, each class as 10x10 heatmaps, \n",
        "# display the same plots as section 2.2.1\n",
        "\n",
        "plt.figure(figsize=(16, 4))\n",
        "\n",
        "for i_class in range(len(class_names)):\n",
        "    i = np.argwhere(y_train == i_class)[1, 0]\n",
        "    ax = plt.subplot(1, len(class_names), i_class + 1)\n",
        "    plt.imshow(encoded_mnist[i].reshape((10, 10)))\n",
        "    plt.gray()\n",
        "    plt.xlabel(class_names[i_class])\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2-hwXu4i2UV"
      },
      "source": [
        "**Summary of 2.2.2**: \n",
        "  \n",
        "From the reconstructed heatmaps for each class of Fashion MNIST dataset with dropout layers applied to incoming weights to all layers\n",
        "except the input and output layers:\n",
        "- I set dropout rate of 0.25, 0.25, and 0.5 for the three dropout layers.\n",
        "- From the training epochs logs above, comparing with the autoencoder without any dropout layers, a greater MSE but higher training accuracy was achieved. \n",
        "- I conclude that the dropout layers worked as regularizer on neural nets that reduces the odds of overfitting by dropping out neurons at random, during every epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsAm9fWvi2db"
      },
      "source": [
        "### 2.2.3 CNN (baseline) \n",
        "Train a convolutional neural network on the training data with the\n",
        "following layer specifications:\n",
        "- 2D convolutional layer, 28 filters, 3x3 window size, ReLU activation\n",
        "- 2x2 max pooling\n",
        "- 2D convolutional layer, 56 filters, 3x3 window size, ReLU activation\n",
        "- fully-connected layer, 56 nodes, ReLU activation\n",
        "- fully-connected layer, 10 nodes, softmax activation\n",
        "\n",
        "Use the Adam optimizer, 32 observations per batch, and sparse categorical cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cODn42j0ijWr"
      },
      "source": [
        "# re-load Fashion MNIST data for new data pre-processing\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "print(\"x_train data shape:\", x_train.shape)\n",
        "print(\"y_train label shape:\", y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTjXSfq_ijVY"
      },
      "source": [
        "# pre-processing\n",
        "# Normalize the data \n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "print(f\"Original x training data size = {x_train.shape}.\")\n",
        "print(f\"Original x test data size = {x_test.shape}.\")\n",
        "\n",
        "# reshape images with suitable input format\n",
        "x_train = x_train.reshape(len(x_train), 28, 28, 1)\n",
        "x_test = x_test.reshape(len(x_test), 28, 28, 1)\n",
        "print(f\"Normalized and reshaped x training data size = {x_train.shape}.\")\n",
        "print(f\"Normalized and reshaped x test data size = {x_test.shape}.\")\n",
        "\n",
        "# use the last 12000 samples of the training data as a validation set\n",
        "x_train = x_train[:-12000]\n",
        "x_valid = x_train[-12000:]  # last 12000 samples\n",
        "y_train = y_train[:-12000]\n",
        "y_valid = y_train[-12000:]  # last 12000 samples\n",
        "print(f\"New training image set size = {x_train.shape}.\")\n",
        "print(f\"New training label set size = {y_train.shape}.\")\n",
        "print(f\"New validation image set size = {x_valid.shape}.\")\n",
        "print(f\"New validation label set size = {y_valid.shape}.\")\n",
        "print(f\"Left test image set size = {x_test.shape}.\")\n",
        "print(f\"Left test label set size = {y_test.shape}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZUXCqX6ijSO"
      },
      "source": [
        "# define the CNN baseline model\n",
        "\n",
        "def cnn_baseline_model(input_shape):\n",
        "    \"\"\" Build the CNN baseline model as provided layer details:\n",
        "    - 2D convolutional layer, 28 filters, 3x3 window size, ReLU activation\n",
        "    - 2x2 max pooling\n",
        "    - 2D convolutional layer, 56 filters, 3x3 window size, ReLU activation\n",
        "    - fully-connected layer, 56 nodes, ReLU activation\n",
        "    - fully-connected layer, 10 nodes, softmax activation\n",
        "    with the Adam optimizer, and sparse categorical cross-entropy loss.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    \n",
        "    # define the input shape in the first layer of the neural network\n",
        "    model.add(layers.Conv2D(28, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
        "    model.add(layers.Conv2D(56, (3, 3), activation='relu', padding='same'))\n",
        "\n",
        "    # transforms the format of the images from a two-dimensional array \n",
        "    # (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels)\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "    model.add(layers.Dense(56, activation='relu'))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    \n",
        "    # compile the model\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss=keras.losses.SparseCategoricalCrossentropy(), \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkfBGtsnijQf"
      },
      "source": [
        "# Print the number of trainable parameters in the model\n",
        "\n",
        "cnn_baseline = cnn_baseline_model(input_shape=(28,28,1))\n",
        "cnn_baseline.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1razv1FijMH"
      },
      "source": [
        "# train the CNN baseline model with 32 observations per batch for 10 epochs\n",
        "\n",
        "history = cnn_baseline.fit(x_train, \n",
        "                           y_train,\n",
        "                           batch_size=32,\n",
        "                           epochs=10,\n",
        "                           shuffle=True,\n",
        "                           validation_data=(x_valid, y_valid),\n",
        "                           verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc5FFpgHijJn"
      },
      "source": [
        "# plot training and validation accuracy as line plots on the same set of axes\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title(\"Model Accuracy\", fontsize=16)\n",
        "plt.xlabel('Epochs', fontsize=14)\n",
        "plt.ylabel('Accuracy', fontsize=14)\n",
        "plt.legend(['Train', 'Test'], fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGvvpEyxijDF"
      },
      "source": [
        "# Evaluating the CNN baseline model\n",
        "\n",
        "score = cnn_baseline.evaluate(x_test, y_test)\n",
        "print('Loss: {:.4f}'.format(score[0]))\n",
        "print('Accuracy: {:.4f}'.format(score[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcqsXNxuijAw"
      },
      "source": [
        "# Show an example from the test set for each class where the model misclassifies\n",
        "\n",
        "# define the label dict\n",
        "cloth_dict = {0 : 'T-shirt/top',\n",
        "            1 : 'Trouser',\n",
        "            2 : 'Pullover',\n",
        "            3 : 'Dress',\n",
        "            4 : 'Coat',\n",
        "            5 : 'Sandal',\n",
        "            6 : 'Shirt',\n",
        "            7 : 'Sneaker',\n",
        "            8 : 'Bag',\n",
        "            9 : 'Ankle boot'}\n",
        "\n",
        "# change y_test to categorical data\n",
        "y_test_cat = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Predict the values from the validation dataset\n",
        "y_pred = cnn_baseline.predict(x_test)\n",
        "\n",
        "# Convert predictions classes and validation observations to one hot vectors\n",
        "y_pred_classes = np.argmax(y_pred, axis=1) \n",
        "y_true = np.argmax(y_test_cat, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vmy7cuxkHl7"
      },
      "source": [
        "incorrect_labels = []  # record index of incorrect labels\n",
        "temp_y_true = []  # record unique class of true labels\n",
        "\n",
        "for i in range(10000):\n",
        "    if not (y_pred_classes[i] == y_true[i]):\n",
        "        if y_true[i] in temp_y_true:\n",
        "            pass\n",
        "        else:\n",
        "            incorrect_labels.append(i)\n",
        "            temp_y_true.append(y_true[i])\n",
        "    if (len(set(temp_y_true)) == 10):\n",
        "        break\n",
        "\n",
        "print(f\"The collected unique misclassified one-hot labels = {incorrect_labels}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L39kodLCkH2t"
      },
      "source": [
        "# make the misclassification matrix\n",
        "\n",
        "fig, ax = plt.subplots(2, 5, figsize=(12, 6))\n",
        "fig.set_size_inches(16, 9)\n",
        "\n",
        "ax[0,0].imshow(x_test[incorrect_labels[0]].reshape(28,28), cmap='gray')\n",
        "ax[0,0].set_title(\"Predicted Label: \" + str(cloth_dict[y_pred_classes[incorrect_labels[0]]]) + \"\\n\"+\"Actual Label: \" + \n",
        "                 str(cloth_dict[y_true[incorrect_labels[0]]]))\n",
        "\n",
        "ax[0,1].imshow(x_test[incorrect_labels[1]].reshape(28,28), cmap='gray')\n",
        "ax[0,1].set_title(\"Predicted Label: \" + str(cloth_dict[y_pred_classes[incorrect_labels[1]]]) + \"\\n\"+\"Actual Label: \" + \n",
        "                 str(cloth_dict[y_true[incorrect_labels[1]]]))\n",
        "\n",
        "ax[0,2].imshow(x_test[incorrect_labels[2]].reshape(28,28), cmap='gray')\n",
        "ax[0,2].set_title(\"Predicted Label: \" + str(cloth_dict[y_pred_classes[incorrect_labels[2]]]) + \"\\n\"+\"Actual Label: \" + \n",
        "                 str(cloth_dict[y_true[incorrect_labels[2]]]))\n",
        "\n",
        "ax[0,3].imshow(x_test[incorrect_labels[3]].reshape(28,28), cmap='gray')\n",
        "ax[0,3].set_title(\"Predicted Label: \" + str(cloth_dict[y_pred_classes[incorrect_labels[3]]]) + \"\\n\"+\"Actual Label: \" + \n",
        "                 str(cloth_dict[y_true[incorrect_labels[3]]]))\n",
        "\n",
        "ax[0,4].imshow(x_test[incorrect_labels[4]].reshape(28,28), cmap='gray')\n",
        "ax[0,4].set_title(\"Predicted Label: \" + str(cloth_dict[y_pred_classes[incorrect_labels[4]]]) + \"\\n\"+\"Actual Label: \" + \n",
        "                 str(cloth_dict[y_true[incorrect_labels[4]]]))\n",
        "\n",
        "ax[1,0].imshow(x_test[incorrect_labels[5]].reshape(28,28), cmap='gray')\n",
        "ax[1,0].set_title(\"Predicted Label: \" + str(cloth_dict[y_pred_classes[incorrect_labels[5]]]) + \"\\n\"+\"Actual Label: \" + \n",
        "                 str(cloth_dict[y_true[incorrect_labels[5]]]))\n",
        "\n",
        "ax[1,1].imshow(x_test[incorrect_labels[6]].reshape(28,28), cmap='gray')\n",
        "ax[1,1].set_title(\"Predicted Label: \" + str(cloth_dict[y_pred_classes[incorrect_labels[6]]]) + \"\\n\"+\"Actual Label: \" + \n",
        "                 str(cloth_dict[y_true[incorrect_labels[6]]]))\n",
        "\n",
        "ax[1,2].imshow(x_test[incorrect_labels[7]].reshape(28,28), cmap='gray')\n",
        "ax[1,2].set_title(\"Predicted Label: \" + str(cloth_dict[y_pred_classes[incorrect_labels[7]]]) + \"\\n\"+\"Actual Label: \" + \n",
        "                 str(cloth_dict[y_true[incorrect_labels[7]]]))\n",
        "\n",
        "ax[1,3].imshow(x_test[incorrect_labels[8]].reshape(28,28), cmap='gray')\n",
        "ax[1,3].set_title(\"Predicted Label: \" + str(cloth_dict[y_pred_classes[incorrect_labels[8]]]) + \"\\n\"+\"Actual Label: \" + \n",
        "                 str(cloth_dict[y_true[incorrect_labels[8]]]))\n",
        "\n",
        "ax[1,4].imshow(x_test[incorrect_labels[9]].reshape(28,28), cmap='gray')\n",
        "ax[1,4].set_title(\"Predicted Label: \" + str(cloth_dict[y_pred_classes[incorrect_labels[9]]]) + \"\\n\"+\"Actual Label: \" + \n",
        "                 str(cloth_dict[y_true[incorrect_labels[9]]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAa4Dy-jkH0d"
      },
      "source": [
        "# Classification Report\n",
        "\n",
        "print(classification_report(y_true, y_pred_classes, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhtgnY2blFqf"
      },
      "source": [
        "**Answer to the questions for 2.2.3**:\n",
        "1. Number of trainable parameters in the CNN baseline model = 629,730, as shown in model summary above.\n",
        "2. Plot of training and validation accuracy at the end of each epoch was shown as above code block.\n",
        "3. Accuracy on the test set = 0.9175, as shown above.\n",
        "4. Figure matrix of an example from the test set for each class where the model misclassifies was shown as above.\n",
        "5. Based on the classification report shown above:\n",
        "  - Trouser and Sandal shared the best precision.\n",
        "  - Trouser, Sandal, Sneaker, and Bag all showed best recall.\n",
        "  - Trouser also showed the best F1 score.\n",
        "  - Shirt showed worst classification statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEKbK3yXlF0B"
      },
      "source": [
        "### 2.2.4 CNN (improvements)\n",
        "\n",
        "- Modify the baseline CNN model to obtain at least 91% accuracy\n",
        "on the test set. \n",
        "- Note the above CNN baseline model already achieved 91% + accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJNW3RAxkHyn"
      },
      "source": [
        "# define the CNN improved model\n",
        "\n",
        "def cnn_improved_model(input_shape):\n",
        "    \"\"\" Build the CNN baseline model as provided layer details:\n",
        "    - 2D convolutional layer, 28 filters, 3x3 window size, ReLU activation\n",
        "    - 2x2 max pooling\n",
        "    - 2D convolutional layer, 56 filters, 3x3 window size, ReLU activation\n",
        "    - fully-connected layer, 56 nodes, ReLU activation\n",
        "    - fully-connected layer, 10 nodes, softmax activation\n",
        "    with the Adam optimizer, and sparse categorical cross-entropy loss.\n",
        "    \"\"\"\n",
        "    model = keras.Sequential()\n",
        "    \n",
        "    # define the input shape in the first layer of the neural network\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2), padding='same'))\n",
        "    model.add(layers.Dropout(0.25))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(layers.Dropout(0.4))\n",
        "\n",
        "    # transforms the format of the images from a two-dimensional array \n",
        "    # (of 28 by 28 pixels) to a one-dimensional array (of 28 * 28 = 784 pixels)\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.3))\n",
        "    model.add(layers.Dense(10, activation='softmax'))\n",
        "    \n",
        "    # compile the model\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss=keras.losses.SparseCategoricalCrossentropy(), \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRe9jtSQkHxO"
      },
      "source": [
        "# Improved CNN model summary\n",
        "\n",
        "cnn_improved = cnn_improved_model(input_shape=(28,28,1))\n",
        "cnn_improved.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8PYTpD_kHv1"
      },
      "source": [
        "# train the CNN improved model with 32 observations per batch for 10 epochs\n",
        "\n",
        "history = cnn_improved.fit(x_train, \n",
        "                           y_train,\n",
        "                           batch_size=64,\n",
        "                           epochs=10,\n",
        "                           shuffle=True,\n",
        "                           validation_data=(x_valid, y_valid),\n",
        "                           verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-F0sMNfkHrQ"
      },
      "source": [
        "# Evaluating the CNN improved model\n",
        "\n",
        "score = cnn_improved.evaluate(x_test, y_test)\n",
        "print('Improved CNN model Loss: {:.4f}'.format(score[0]))\n",
        "print('Improved CNN model Accuracy: {:.4f}'.format(score[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw87nOD7m8FA"
      },
      "source": [
        "**Improved CNN Model Summary 2.2.4**:\n",
        "- The improved CNN model has test set accuracy = 0.9270.\n",
        "- The detailed model architecture and hyperparameters were implemented inside of the function `cnn_improved_model`.\n",
        "- The justifications were based on:\n",
        "  - Adding Dropout layers before Conv2D layer, Flatten layer, and fully-connected layer.\n",
        "  - Increase the number of filters in Conv2D layers and fully-connected layers.\n",
        "  - Increase the batch size from 32 to 64."
      ]
    }
  ]
}